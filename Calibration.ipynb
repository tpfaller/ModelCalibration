{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=100_000, n_features=20, n_informative=2, n_redundant=2, random_state=42\n",
    ")\n",
    "\n",
    "train_samples = 100  # Samples used for training the models\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    shuffle=False,\n",
    "    test_size=100_000 - train_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[y==0,4],X[y==0,3],alpha=0.1)\n",
    "plt.scatter(X[y==1,4],X[y==1,3],alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create classifiers\n",
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "clf_list = [\n",
    "    (lr, \"Logistic\"),\n",
    "    (gnb, \"Naive Bayes\"),\n",
    "    (rfc, \"Random forest\"),\n",
    "]\n",
    "\n",
    "for clf, name in clf_list:\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.histogram(rfc.predict_proba(X_test)[y_test==1,1],np.linspace(0,1,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.histogram(rfc.predict_proba(X_test)[y_test==0,1],np.linspace(0,1,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "prob_pos, probs = calibration_curve(y_test, rfc.predict_proba(X_test)[:,1], n_bins=20)\n",
    "\n",
    "plt.plot(probs,prob_pos)\n",
    "plt.plot(probs[10],prob_pos[10],'r*')\n",
    "plt.xlabel('Mittlere gesch√§tzte Wahrscheinlichkeit')\n",
    "plt.ylabel('Mittlere accuracy')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rfc.predict_proba(X_test)[y_test==0,1],np.linspace(0,1,21), alpha=0.5, label='y==0')\n",
    "plt.hist(rfc.predict_proba(X_test)[y_test==1,1],np.linspace(0,1,21), alpha=0.5,  label='y==1')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = GridSpec(4, 2)\n",
    "colors = plt.get_cmap(\"Dark2\")\n",
    "\n",
    "ax_calibration_curve = fig.add_subplot(gs[:2, :2])\n",
    "calibration_displays = {}\n",
    "markers = [\"^\", \"v\", \"s\", \"o\"]\n",
    "for i, (clf, name) in enumerate(clf_list):\n",
    "    \n",
    "    display = CalibrationDisplay.from_estimator(\n",
    "        clf,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        n_bins=20,\n",
    "        name=name,\n",
    "        ax=ax_calibration_curve,\n",
    "        color=colors(i),\n",
    "        marker=markers[i],\n",
    "    )\n",
    "    calibration_displays[name] = display\n",
    "\n",
    "ax_calibration_curve.grid()\n",
    "ax_calibration_curve.set_title(\"Calibration plots\")\n",
    "\n",
    "# Add histogram\n",
    "grid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\n",
    "for i, (_, name) in enumerate(clf_list):\n",
    "    row, col = grid_positions[i]\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "\n",
    "    ax.hist(\n",
    "        calibration_displays[name].y_prob,\n",
    "        range=(0, 1),\n",
    "        bins=20,\n",
    "        label=name,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "nn = torch.nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 32) \n",
    "        self.fc2 = nn.Linear(32, 32) \n",
    "        self.fc3 = nn.Linear(32, 32) \n",
    "        self.fc4 = nn.Linear(32, 2) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sigmoid(self.fc1(x))\n",
    "        out = self.sigmoid(self.fc2(out))\n",
    "        out = nn.Dropout()(out)\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        out = nn.Dropout()(out)\n",
    "        out = self.sigmoid(self.fc4(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "n_iter = 8000\n",
    "\n",
    "lossvals=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(torch.Tensor(X_train).to(DEVICE))\n",
    "    target=torch.stack([1-torch.Tensor(y_train), torch.Tensor(y_train)],axis=1).squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target)\n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(torch.Tensor(X_test).to('cpu'))\n",
    "fpr,tpr,thresholds = roc_curve(y_test, y_test_pred[:,1].cpu().detach().numpy() )\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('False-Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True-Positive Rate (Sensitivity)')\n",
    "plt.show()\n",
    "\n",
    "print('ROC AUC score: {:.3f}'.format(roc_auc_score(y_test, y_test_pred[:,1].cpu().detach().numpy())))\n",
    "\n",
    "print(\"Accuracy: {:.4f}, F1 Score: {:.4f}\".format(accuracy_score(y_test, y_test_pred.cpu().argmax(1)), f1_score(y_test, y_test_pred.cpu().argmax(1))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = CalibrationDisplay.from_predictions(\n",
    "        y_test,\n",
    "        y_test_pred.cpu().numpy()[:,1],\n",
    "        n_bins=20,\n",
    "        name='MLP',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "n_iter = 2000\n",
    "\n",
    "lossvals=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(torch.Tensor(X_train).to(DEVICE))\n",
    "    target=torch.stack([1-torch.Tensor(y_train), torch.Tensor(y_train)],axis=1).squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target)\n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it%100==0:\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(torch.Tensor(X_test).to(DEVICE))\n",
    "        \n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "                y_test,\n",
    "                y_test_pred.cpu().numpy()[:,1],\n",
    "                n_bins=20,\n",
    "                name='MLP',\n",
    "            )\n",
    "        plt.title(f'N={it} iterations')\n",
    "        # plt.savefig(f'calibration_{it}_it.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "n_iter = 2000\n",
    "alpha=0.01\n",
    "lossvals=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(torch.Tensor(X_train).to(DEVICE))\n",
    "    target=torch.stack([1-torch.Tensor(y_train), torch.Tensor(y_train)],axis=1).squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target) + alpha*torch.Tensor([x.abs().mean() for x in model.parameters()]).mean()\n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it%100==0:\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(torch.Tensor(X_test).to(DEVICE))\n",
    "        \n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "                y_test,\n",
    "                y_test_pred.cpu().numpy()[:,1],\n",
    "                n_bins=20,\n",
    "                name='MLP',\n",
    "            )\n",
    "        plt.title(f'N={it} iterations')\n",
    "        # plt.savefig(f'calibration_l1_{it}_it.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class LogitNormLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, device, t=1.0):\n",
    "        super(LogitNormLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.t = t\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        norms = torch.norm(x, p=2, dim=-1, keepdim=True) + 1e-7\n",
    "        logit_norm = torch.div(x, norms) / self.t\n",
    "        return F.cross_entropy(logit_norm, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "loss_fn = LogitNormLoss(device=DEVICE)\n",
    "n_iter = 2000\n",
    "alpha=0.01\n",
    "lossvals=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(torch.Tensor(X_train).to(DEVICE))\n",
    "    target=torch.stack([1-torch.Tensor(y_train), torch.Tensor(y_train)],axis=1).squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target) \n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it%100==0:\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(torch.Tensor(X_test).to(DEVICE))\n",
    "        \n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "                y_test,\n",
    "                y_test_pred.cpu().numpy()[:,1],\n",
    "                n_bins=20,\n",
    "                name='MLP',\n",
    "            )\n",
    "        plt.title(f'N={it} iterations')\n",
    "        # plt.savefig(f'calibration_softlogits_{it}_it.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]\n",
    "        target_new = (1-self.epsilon)*target + self.epsilon/n\n",
    "        return F.cross_entropy(preds, target_new)\n",
    "\n",
    "l= LabelSmoothingCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.2)\n",
    "loss_fn = LabelSmoothingCrossEntropy(epsilon=0.01)\n",
    "n_iter = 2000\n",
    "alpha=0.01\n",
    "lossvals=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(torch.Tensor(X_train).to(DEVICE))\n",
    "    target=torch.stack([1-torch.Tensor(y_train), torch.Tensor(y_train)],axis=1).squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target) \n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it%100==0:\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(torch.Tensor(X_test).to(DEVICE))\n",
    "        \n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "                y_test,\n",
    "                y_test_pred.cpu().numpy()[:,1],\n",
    "                n_bins=20,\n",
    "                name='MLP',\n",
    "            )\n",
    "        plt.title(f'N={it} iterations')\n",
    "        # plt.savefig(f'calibration_labelsmoothing_{it}_it.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops.focal_loss import sigmoid_focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "n_iter = 2000\n",
    "alpha=0.01\n",
    "lossvals=[]\n",
    "lossvals_val=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(torch.Tensor(X_train).to(DEVICE))\n",
    "    target=torch.stack([1-torch.Tensor(y_train), torch.Tensor(y_train)],axis=1).squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target) \n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it%50==0:\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(torch.Tensor(X_test).to(DEVICE))\n",
    "        \n",
    "            loss = loss_fn(y_test_pred, torch.stack([1-torch.Tensor(y_test), torch.Tensor(y_test)],axis=1).squeeze().to(DEVICE)) \n",
    "            lossvals_val.append(loss.detach().cpu().numpy().tolist())\n",
    "#        plt.title(f'N={it} iterations')\n",
    "#        plt.savefig(f'calibration_labelsmoothing_{it}_it.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossvals,label='train')\n",
    "plt.plot(np.arange(0,2000,50),lossvals_val, label='val')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "from torchvision.transforms import v2\n",
    "mixup=v2.MixUp(alpha=2.0, num_classes=2)\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "n_iter = 2000\n",
    "alpha=0.01\n",
    "lossvals=[]\n",
    "lossvals_val=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    X_train_mixup, y_train_mixup = mixup(torch.Tensor(X_train).view(-1,1,1,20), torch.Tensor(y_train).long())\n",
    "    X_train_mixup = X_train_mixup.view(-1,20)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_mixup.to(DEVICE))\n",
    "    target=y_train_mixup.squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target) \n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it%50==0:\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(torch.Tensor(X_test).to(DEVICE))\n",
    "        \n",
    "            loss = loss_fn(y_test_pred, torch.stack([1-torch.Tensor(y_test), torch.Tensor(y_test)],axis=1).squeeze().to(DEVICE)) \n",
    "            lossvals_val.append(loss.detach().cpu().numpy().tolist())\n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "                y_test,\n",
    "                y_test_pred.cpu().numpy()[:,1],\n",
    "                n_bins=20,\n",
    "                name='MLP',\n",
    "            )\n",
    "        plt.title(f'N={it} iterations')\n",
    "        # plt.savefig(f'calibration_mixup_alpha_2_{it}_it.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "from torchvision.transforms import v2\n",
    "mixup=v2.MixUp(alpha=1.0, num_classes=2)\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "DEVICE='cpu' # set to mps for Apple M1/M2/M3, set to cuda if available, else set to cpu\n",
    "# Creating a model\n",
    "model = MLP().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "n_iter = 2000\n",
    "alpha=0.01\n",
    "lossvals=[]\n",
    "lossvals_val=[]\n",
    "for it in tqdm(range(n_iter)):\n",
    "    X_train_mixup, y_train_mixup = mixup(torch.Tensor(X_train).view(-1,1,1,20), torch.Tensor(y_train).long())\n",
    "    X_train_mixup = X_train_mixup.view(-1,20)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_mixup.to(DEVICE))\n",
    "    target=y_train_mixup.squeeze().to(DEVICE)\n",
    "    loss = loss_fn(predictions, target) \n",
    "    lossvals.append(loss.detach().cpu().numpy().tolist()) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it%100==0:\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(torch.Tensor(X_test).to(DEVICE))\n",
    "        \n",
    "            loss = loss_fn(y_test_pred, torch.stack([1-torch.Tensor(y_test), torch.Tensor(y_test)],axis=1).squeeze().to(DEVICE)) \n",
    "            lossvals_val.append(loss.detach().cpu().numpy().tolist())\n",
    "        display = CalibrationDisplay.from_predictions(\n",
    "                y_test,\n",
    "                y_test_pred.cpu().numpy()[:,1],\n",
    "                n_bins=20,\n",
    "                name='MLP',\n",
    "            )\n",
    "        plt.title(f'N={it} iterations')\n",
    "        plt.savefig(f'calibration_mixup_alpha_1_{it}_it.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup=v2.MixUp(alpha=1.0, num_classes=2)\n",
    "y_train_vals = []\n",
    "for k in range(1000):\n",
    "    y_train_vals += y_train.tolist()\n",
    "#plt.hist(y_train_mixup[:,0],20,alpha=0.5, label='class 0')\n",
    "#plt.hist(y_train_mixup[:,1],20,alpha=0.5, label='class 1')\n",
    "plt.hist(y_train_vals,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup=v2.MixUp(alpha=2.0, num_classes=2)\n",
    "y_train_vals = []\n",
    "for k in range(1000):\n",
    "    X_train_mixup, y_train_mixup = mixup(torch.Tensor(X_train).view(-1,1,1,20), torch.Tensor(y_train).long())\n",
    "    y_train_vals += y_train_mixup[:,0].cpu().tolist()\n",
    "#plt.hist(y_train_mixup[:,0],20,alpha=0.5, label='class 0')\n",
    "#plt.hist(y_train_mixup[:,1],20,alpha=0.5, label='class 1')\n",
    "plt.hist(y_train_vals,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mixup, y_train_mixup = mixup(torch.Tensor(X_train).view(-1,1,1,20), torch.Tensor(y_train).long())\n",
    "X_train_mixup = X_train_mixup.view(-1,20)\n",
    "plt.hist(y_train_mixup[:,0],20,alpha=0.5, label='class 0')\n",
    "plt.hist(y_train_mixup[:,1],20,alpha=0.5, label='class 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
